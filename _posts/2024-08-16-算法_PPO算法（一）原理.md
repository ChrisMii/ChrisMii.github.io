---
layout:     post
title:      LLM中的PPO算法
subtitle:   白话原理
date:       2024-08-16
author:     BY
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Blog
---

### PPO in InstructGPT
大语言模型的常用训练有三个阶段，分别是预训练、监督微调和对齐。InstructGPT在论文中提到对齐阶段使用了PPO算法，文中的损失函数形式如下：
$$
    \max_{\pi_{\theta}}\mathbb{E}_{x \sim D,y \sim \pi_{\theta}(y|x)}[r_{\phi}(x,y)]-\beta\mathbb{D}_{KL}[\pi_{\theta}||\pi_{ref}]+\gamma\mathbb{E}_{x \sim D_{pretrain}}[log(\pi_{\theta}(x))]
$$
其中，
$
\pi_{\theta}
$
是待训练的policy model，
$
\pi_{ref}
$
是监督微调过的模型，
$
r_{\phi}
$
是已经训练好的reward model。实际上，这个公式第一项才是PPO算法的表达式，二三项都是额外增加的约束。那么第一项到底是长什么样的呢？

### PPO
实际使用的PPO目标函数如下：
$$
J_{ppo}(\theta)=\mathbb{E}[x \sim D, y \sim \pi_{\theta_{old}}(y|x)]\frac{1}{|y|}\sum_{t=1}^{|y|}\min[\frac{\pi_{\theta}(y_t|x,y_{<t})}{\pi_{\theta_{old}}(y_t|x,y_{<t})}A_t, clip(\frac{\pi_{\theta}(y_t|x,y_{<t})}{\pi_{\theta_{old}}(y_t|x,y_{<t})},1-\epsilon,1+\epsilon)A_t]
$$
其中，
$
\pi_{\theta}
$
是当前的policy model和
$
\pi_{\theta_{old}}
$
上一次迭代的老的policy model。
$
A_t
$
是优势函数，基于奖励和值函数通过GAE方法(Generalized Advantage Estimation)计算得到。其中奖励的计算为：
$$
r_t=r_{\phi}(x,y_{\le t})-\beta log\frac{\pi_{\theta}(y_t|x, y \lt t)}{\pi_{ref}(y_t|x, y \lt t)}
$$

### PPO训练
具体的训练包含四步：收集数据、计算优势、更新Actor、更新Critic。

>1.收集数据
>
>概括地说，在环境中让基于当前策略进行交互，产生一些反馈，包括状态、动作、奖励。
>
>a) 输入prompt，使用actor生成response
>
>b) 拼接prompt和response，使用reward model得到最后一个token的reward
>
>c) 拼接prompt和response，使用critic model得到每个token对应的q value
>
>d) 拼接prompt和response，使用actor和reference model，分别得到response部分的log_probs

>2.计算优势
>
>优势函数
$
A(s,a)=Q(s,a)-V(s)=TDError=r+\gamma V(s^{'})-V(s)
$
>
>$
A_t=\alpha A_{t+1}+\alpha^2 A_{t+2} + ···
>$

>3.更新Actor
> 
>$
>J_{ppo}(\theta)=\min(r_t(\theta)\hat{A}_t,clip(r_t(\theta),1+\epsilon,1-\epsilon)\hat{A}_t), where\,\,r_t(\theta)=\frac{\pi_{\theta}(a_t|s_t)}{\pi_{old}(a_t|s_t)}
>$
>
>a) 拼接prompt和response，使用actor，得到每个token的log_probs
>b) 计算ratio $r_t$和loss
>c) 更新Actor

>4.更新Critic
>计算return(discount accumulative reward)和value之间的mse差异作为loss，更新Critic。实际训练中，两个loss是加权后一起训练的。

