---
layout:     post
title:      多模态向量模型
subtitle:   General MultiModal Embedding(GME)
date:       2024-08-22
author:     BY
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Blog
---

### Open Datasets

| class | Task | Datasets| Domain |
| - | - | - | - |
| Single-Modal | T $\rightarrow$ T | ArguAna(2020) | |
| Single-Modal | T $\rightarrow$ T | Climate-FEVER(2020) | |
| Single-Modal | T $\rightarrow$ T | CQADupStack(2015) | |
| Single-Modal | T $\rightarrow$ T | DBPeia(2017) |
| Single-Modal | T $\rightarrow$ T | FEVER(2018) |
| Single-Modal | T $\rightarrow$ T | FiQA2018(2018) |
| Single-Modal | T $\rightarrow$ T | HotpotQA(2018) |
| Single-Modal | T $\rightarrow$ T | MSMARCO(2016) |
| Single-Modal | T $\rightarrow$ T | NFCorpus(2016) |
| Single-Modal | T $\rightarrow$ T | NQ(2019) |
| Single-Modal | T $\rightarrow$ T | Quora SCIDOCS(2020) |
| Single-Modal | T $\rightarrow$ T | SciFact(2020) |
| Single-Modal | T $\rightarrow$ T | Touche2020(2020) |
| Single-Modal | T $\rightarrow$ T | TRECCOVID(2020) |
| Single-Modal | T $\rightarrow$ T | WebQA(2022) |
| Single-Modal | I $\rightarrow$ I | Nights(2023) |
| Cross-Modal | T $\rightarrow$ I | VisualNews(2021a) |
| Cross-Modal | T $\rightarrow$ I | Fashion200k(2017) |
| Cross-Modal | T $\rightarrow$ I | MSCOCO(2014) |
| Cross-Modal | T $\rightarrow$ I | Flickr30k(2015) |
| Cross-Modal | T $\rightarrow$ VD | TAT-DQA(2022) |
| Cross-Modal | T $\rightarrow$ I | ArxivQA (2024) |
| Cross-Modal | T $\rightarrow$ I | DocVQA(2021) |
| Cross-Modal | T $\rightarrow$ I | Shift Project,Artificial Intelligence, Government Reports, Healthcare Industry, Energy, TabFQuad(2024a) |
| Fused-Modal | T $\rightarrow$ IT | WebQA(2022) |
| Fused-Modal | T $\rightarrow$ IT | EDIS(2023b) |
| Fused-Modal | IT $\rightarrow$ T | OVEN(2023) |
| Fused-Modal | IT $\rightarrow$ T | INFOSEEK(2023) |
| Fused-Modal | IT $\rightarrow$ T | ReMuQ(2023) |
| Fused-Modal | IT $\rightarrow$ T | QKVQA(2019) |
| Fused-Modal | IT $\rightarrow$ T | LLaVA(2024) |
| Fused-Modal | IT $\rightarrow$ I | FasionIQ(2021) |
| Fused-Modal | IT $\rightarrow$ I | CIRR(2021b) |
| Fused-Modal | IT $\rightarrow$ IT | OVEN(2023) |
| Fused-Modal | IT $\rightarrow$ IT | EVQA(2023) |
| Fused-Modal | IT $\rightarrow$ IT | INFOSEEK(2023) |



### Fused-Modal Data Synthesis
Open fused-modal datasets are limited in domains and quantitives. So the author propose to synthesize more data efficiently. Wiki has [doc, img] data, and pair them with synthesized [query, img] data. The synthesis pipeline are the following steps.
1. prepare the candidate IT data
```
WikiWeb2M ————（classify docs of wiki into several domains and sample docs uniformly from these domains. Keep data with classification confidences scores above 0.5）————> [doc,image] of 313,284
```
2. construct T $\rightarrow$ IT data
``` 
doc ————（generate a query by LLM| filter by q-to-doc hit@20）————> q
q ————（extract entity and rewrite query by LLM）————> entity, q'
```
3. construct IT $\rightarrow$ IT data
Supplement the image data on the query q' side.
```
a) entity ————（Google Image Search API and retain top5）————> imgs (what about relevance between q' and img?)
b) entity+doc ————（generate caption by LLM）————> caption ————（generate images by FLUX.1-dev）————> imgs
```
4. data filtering
FLUX provides consistent quality data whereas Google often included noisy data which needs to be filtered.
```
[(img1, doc), q, (caption, img2)] ————（filter by img2-caption relevance below 0.2 by clip-vit-large-patch14）———— [(img1, doc), q, (caption, img2)]
```
Before filtering, produced 135,000 high-quality fuse-modal traininng data(incluing T$\rightarrow$IT and IT$\rightarrow$IT types). And retained 1,102,000 after filtering.

The entire process consumed 600 A100 GPU hours.