---
layout:     post
title:      DeepSeek-R1极简理解
subtitle:   概览R1都做了什么
date:       2025-01-22
author:     BY
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Blog
---

### R1-Zero流程
```
V3-Base ——(RL on supervised data | reward=accuracy+format)——> R1-Zero
```
### R1流程

#### 1. enhance readability
```
R1-Zero ——(generate data and filter by correctness)——> cold start data
V3-Base ——(finetune on cold start data)——> cold start model
    cold start model ——(RL on supervised data| reward=accuracy+language consistancy)—— rl model
```
#### 2. enchance general ability besides reasoning
```
rl model ——(generate data and filter by correctness and V3 judge)——> reasoning data
V3 ——(reuse SFT data of V3 and generate CoT for part of it)——> non reasoning data
V3-Base ——(finetune on above 2 data for 2 epochs)——> sft model
```
#### 3. futher align the human preference
```
sft model ——(RL on 2 data| reward=accuracy+reward model)——> R1
```
### Distilation
Only apply sft other smaller model on the 2 sft data above and boost the performance.

### Unsuccessful Attempts
#### PRM
limitations:
1. challenging to define a fine-grain step
2. hard to determine the correctness of intermediate step
3. reward hacking and overburden of retraining PRM
Not suitable for large-scale reinforcement learning.

#### MCTS
limitations:
1. too large search space in text generation
2. rely on a fine-grained value model which is hard to obtain
Iteratively boosting model performance via self-search remains challenging.

### Future
1. to enhance on general tasks.
2. to expand on other language.
3. to solve the prompt sensitive problem.
4. to include software engineering tasks and imporve the evaluation efficiency.