---
layout:     post
title:      理解GRPO算法
subtitle:   简明
date:       2025-02-06
author:     BY
header-img: img/post-bg-re-vs-ng2.jpg
catalog: true
tags:
    - Blog
---
GRPO是deepseek提出的一种强化学习算法，来自《DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models》，我们来看看它与PPO算法到底有什么区别。
#### 损失函数
PPO的损失函数如下：
$$
J_{PPO}(\theta)=\mathbb{E}[q\sim P(Q),o\sim \pi_{\theta_{old}}(O|q)]\frac{1}{|o|}\sum_{t=1}^{|o|}\min[\frac{\pi_{\theta}(o_t|q,o_{<t})}{\pi_{\theta_{old}}(o_t|q,o_{<t})}A_t,\ clip(\frac{\pi_{\theta}(o_t|q,o_{<t})}{\pi_{\theta_{old}}(o_t|q,o_{<t})},1+\epsilon,1-\epsilon)A_t]
$$

GRPO的损失函数如下：
$$
J_{GRPO}(\theta)=\mathbb{E}[q\sim P(Q),{\{o^i\}}_{i=1}^{G}\sim \pi_{\theta_{old}}(O|q)]\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o^i|}\sum_{t=1}^{|o^i|}\{\min[\frac{\pi_{\theta}(o^{i}_{t}|q,o^{i}_{<t})}{\pi_{\theta_{old}}(o^i_t|q,o^i_{<t})}A^i_t,\ clip(\frac{\pi_{\theta}(o^i_t|q,o^i_{<t})}{\pi_{\theta_{old}}(o^i_t|q,o^i_{<t})},1+\epsilon,1-\epsilon)A^i_t] -\beta \mathbb{D}_{KL}[\pi_{\theta}||\pi_{ref}]\}
$$

直观对比两个损失，主要差别在1）GRPO输出带有了分组编号i，并在组内先平均，再在组间平均，并配合重新定义优势函数；2）GRPO多了个KL的计算项。具体区别如下。

1）分组

分组是GRPO用来简化PPO中value function的主要方式，对于每条样本，它会采样G个输出，通过这G个输出对应reward的均值，作为baseline来替代value function的作用。在强化学习训练中，引入baseline可以降低方差，在GAE中，定义了一个优势函数来实现目的，$A_t=r_t+\gamma V(s_{t+1})-V(s_t)$，其中V作为value function需要额外的模型来进行估计，因此带来了额外的成本。在GRPO中，通过计算组内奖励的统计均值，避开了使用模型估计value function。其优势函数的定义如下，当使用ORM时，

$$
A_{i,t}=\frac{r_i-mean([r_1,...,r_G])}{std([r_1,...,r_G])}
$$

当使用PRM时，

$$
A_{i,t}=\sum_{index(j)\ge t}\frac{r_i^{index(j)}-mean([r_1,...,r_G])}{std([r_1,...,r_G])}
$$

其中$r_i=\{r_i^{index(1)},...,r_i^{index(K_i)}\}$，$r_i^{index(j)}$表示第i个输出的第j步的奖励，$index(j)$表示第j步的最后一个token的索引。直白来说，就是把当前位置t后的所有step的奖励分别归一化然后加起来作为当前位置的优势。


2）KL约束

在PPO中，将KL约束加在了奖励中，$r_t=r_\phi(q,o_{\le t})-\beta log \frac{\pi_\theta(o_t\\|q,o_{\lt t})}{\pi_{ref}(o_t\\|q, o \lt t)}$，其中$\pi_{ref}$是reference model，一般是SFT模型，用来约束模型与它的偏差不要过大。$r_\phi$是奖励模型。在GRPO中，对KL形式做了两方面修改，一是将其从优势函数中挪了出来单独计算(其实跟PPO放进优势函数中计算是等价的)，二是使用了无偏形式$\mathbf{D}_{KL}[\pi_\theta||\pi_{ref}]=\frac{\pi_{ref}(o^i_{t}\\|q,o^i_{<t})}{\pi_\theta(o^i_{t}\\|q,o^i_{<t})}-log\frac{\pi_{ref}(o^i_{t}\\|q,o^i_{<t})}{\pi_\theta(o^i_{t}\\|q,o^i_{<t})}-1$，并且保证是正的。

#### 发现
上面就是GRPO相对于PPO做的主要调整。GRPO不仅省去了额外的模型开销，还能更快地稳定得收敛。同时作者还从梯度角度，提供了一个SFT/RFT/DPO/PPO/GRPO等方法的统一的分析视角，即：

$$
    \nabla_\theta J_A(\theta)=\mathbb{E}[\underbrace{(q,o) \sim D}_{Data\ source}](\frac{1}{|o|}\sum_{t=1}^{|o|}\underbrace{GC_A(q,o,t,\pi_{rf})}_{Gradient\ Coefficient}\nabla_\theta log \pi_\theta(o_t|q,o_{\lt t}))
$$

其中，$D$表示数据集，$A$表示将数据和奖励信号转换成能表示对数据进行加强或者惩罚的程度，$\pi_{rf}$表示奖励信号。

![统一视角下的不同方法](https://github.com/ChrisMii/ChrisMii.github.io/tree/master/img/post_related/grpo_tabe10.jpg "Magic Gardens")

表中的梯度系数GC没有贴出来，感兴趣的可以查看原文。